{
    "class": "GPTDataset",
    "dataset_path": "/kimchou/Megatron-LM-HDU/datasets/wikitext2_text_document",
    "num_samples": 400,
    "index_split": "train",
    "random_seed": 1234,
    "sequence_length": 1024,
    "split": "949,50,1",
    "split_matrix": [
        [
            0,
            0.949
        ],
        [
            0.949,
            0.999
        ],
        [
            0.999,
            1.0
        ]
    ],
    "tokenizer": {
        "class": "_Llama2Tokenizer",
        "tokenizer_path": [
            "/kimchou/Megatron-LM-HDU/mixtral/tokenizer.model"
        ],
        "vocab_extra_ids": "0"
    }
}